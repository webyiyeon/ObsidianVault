### 토큰(Token)
토큰(Token)은 자연어 처리 (NLP)에서 텍스트를 작은 단위로 나누어 처리하기 위해 사용되는 기본 단위입니다. 단어, 부분 단어, 문자 등이 토큰이 될 수 있습니다.
	LLM(대형 언어 모델)에서 토큰은 텍스트 데이터를 모델이 이해하고 처리하기 위해 분할된 기본 단위입니다. 텍스트를 토큰으로 나누는 과정을 '토큰화'라고 합니다.
#### 토큰화의 방법
토큰화 방법에는 여러 가지가 있으며, 사용하는 방법에 따라 토큰의 정의가 달라질 수 있습니다.
- 문자 기반 토큰화: 텍스트를 문자 단위로 나누는 방법
	- 예시: "Hello" → ["H", "e", "l", "l", "o"]
- 단어 기반 토큰화: 텍스트를 단어 단위로 나누는 방법
	- 예시: "Hello, world" → ["Hello", ",", "wordl", "!"]
- **서브워드 기반 토큰화**: 단어를 더 작은 단위(subword)로 나누는 방법. **자주 사용되는 서브워드**를 기준으로 분할
	- 예시: "unhappiness" → ["un", "happiness"]
	- [참고] BPE(Byte Pair Encoding): 자주 등장하는 문자 쌍을 합쳐가며 서브워드를 생성하는 알고리즘 
	==GPT → 다음 Token 을 예측하는 것 → hallucination (거짓말, 헛소리)==

#### 토큰의 중요성
토큰은 모델이 텍스트를 이해하고 처리하는데 핵심적인 역할을 합니다. 토큰화의 결과에 따라 모델의 성능이 크게 영향을 받을 수 있습니다. 잘 정의된 토큰화 방법을 사용하면 모델이 텍스트의 의미를 더 정확하게 파악할 수 있습니다.
- **문맥 이해**: 모델이 문맥을 이해하고 적절하게 응답할 수 있게 도와준다.
- **효율성**: 적절한 크기의 토큰을 사용함으로써 연산 자원을 효율적으로 사용할 수 있다.

https://tiktokenizer.vercel.app/

