### 토큰(Token)
토큰(Token)은 자연어 처리 (NLP)에서 텍스트를 작은 단위로 나누어 처리하기 위해 사용되는 기본 단위입니다. 단어, 부분 단어, 문자 등이 토큰이 될 수 있습니다.
	LLM(대형 언어 모델)에서 토큰은 텍스트 데이터를 모델이 이해하고 처리하기 위해 분할된 기본 단위입니다. 텍스트를 토큰으로 나누는 과정을 '토큰화'라고 합니다.
#### 토큰화의 방법
토큰화 방법에는 여러 가지가 있으며, 사용하는 방법에 따라 토큰의 정의가 달라질 수 있습니다.
- 문자 기반 토큰화: 텍스트를 문자 단위로 나누는 방법
	- 예시: "Hello" → ["H", "e", "l", "l", "o"]
- 단어 기반 토큰화: 텍스트를 단어 단위로 나누는 방법
	- 예시: "Hello, world" → ["Hello", ",", "wordl", "!"]
- **서브워드 기반 토큰화**: 단어를 더 작은 단위(subword)로 나누는 방법. **자주 사용되는 서브워드**를 기준으로 분할
	- 예시: "unhappiness" → ["un", "happiness"]
	- [참고] BPE(Byte Pair Encoding): 자주 등장하는 문자 쌍을 합쳐가며 서브워드를 생성하는 알고리즘 
	==GPT → 다음 Token 을 예측하는 것 → hallucination (거짓말, 헛소리)==

#### 토큰의 중요성
토큰은 모델이 텍스트를 이해하고 처리하는데 핵심적인 역할을 합니다. 토큰화의 결과에 따라 모델의 성능이 크게 영향을 받을 수 있습니다. 잘 정의된 토큰화 방법을 사용하면 모델이 텍스트의 의미를 더 정확하게 파악할 수 있습니다.
- **문맥 이해**: 모델이 문맥을 이해하고 적절하게 응답할 수 있게 도와준다.
- **효율성**: 적절한 크기의 토큰을 사용함으로써 연산 자원을 효율적으로 사용할 수 있다.

https://tiktokenizer.vercel.app/


---

## Quantization (양자화)
예시: llama3-8B, 70B, 405B 
	→ parameter 숫자 
		⇒ 뇌의 뇌세포처럼 GPT 모델에 뉴런 존재 
		⇒ 숫자 (가중치) weight 
		⇒ 뇌세포가 많을수록 똑똑한 것 처럼 노드가 많을수록 똑똑할 것으로 추정 
		"학습한다" → 가중치를 조정하여 더 정확한 답이 나올 수 있도록 해주는 것 
		→ 메모리를 엄청나게 차지
	⇒ 그래서 '양자화' 적용
	10.8B 되는 뉴런을 근사치로 변경 → 단순해짐 
	
| 0   |     |     |     |     |     | 1   |
| --- | --- | --- | --- | --- | --- | --- |
| 0   |     |     |     | 1   |     |     |
| 0   |     | 1   |     |     |     |     |

---

## RAG 를 사용해야 하는 이유
### RAG(Retrieval-Augmented Generation) - 검색, 증강, 생성
#### RAG 기술이 주목받고 있는 이유

##### AS-IS
현재의 ChatGPT가 가질 수 있는 문제점
1. 최신 정보에 대한 학습이 되어있지 않음
2. 나(개인) 혹은 우리 회사에 제한되어있는 내부 데이터에 대한 학습이 되어있지 않음
3. 따라서, 특정 도메인(나의 개인정보, 회사의 내부 정보)에 대한 질문을 하면 기대하는 답변을 얻을 수 없음
4. 문서화시켜 업로드를 ChatGPT에 질의할 수 있지만, 기대하는 답변을 받을 수 없거나, 할루시네이션 현상이 발생.
   게다가 문서의 양이 많아지면 이러한 현상은 더욱더 심해짐

##### TO-BE
적합한 RAG를 적용했을 때는,
1. 최신 정보를 기반으로 답변 가능. "검색" 기능 활용 답변 가능
2. 나(개인) 혹은 우리 회사에 제한되어 있는 내부 데이터를 참고하여 답변 가능 
3. 문서를 내부 DB에 저장할 수 있고, DB에 내용을 축적해나갈 수 있으며, 저장된 DB에서 원하는 정보를 검색하여 검색된 정보를 바탕으로 답변 가능
4. 답변에 대한 출처를 역으로 저장되어있는 DB에서 검색 후 검증하는 방식으로 할루시네이션 현상을 줄일 수 있음

궁극적으로 더 나은 답변 품질을 기대할 수 있으며, 방대한 지식 기반으로 답변하는 도메인 특화 챗봇 생성 가능 


#### RAG 프로세스
![[RAG process]]


#### 최적화 플로우
![[Optimization Flow]]


#### RAG의 8단계 프로세스
##### 사전 준비단계
![[Pasted image 20240724123921.png]]
1. 문서로드(Document Load): 문서 내용을 불러옵니다.
2. 분할(Text Split): 문서를 특정 기준(Chunk) 으로 분할합니다.
3. 임베딩(Embedding): 분할된(Chunk) 를 임베딩하여 저장합니다.
4. 벡터DB 저장: 임베딩된 Chunk 를 DB에 저장합니다.

- SPLIT 하는 이유
	1. LLM 이 모든 문서의 내용을 전달 받기 어려울 수 있음. input token size, 입력으로 받을 수 있는 토큰에 제한이 있음.
	2. 문서 내에 답변에 필요한 정보는 문서 전체 중에서 일부인 경우가 많음. 필요한 정보를 골라서 삽입하는 것이 유리.
- 임베딩(Embedding)
	- 문서분할 단계에서 생성된 문서 단위들을 기계가 이해할 수 있는 수치적 형태로 변환하는 과정. 이 단계는 RAG 시스템의 핵심적인 부분 중 하나로, 문서의 의미를 벡터(숫자의 배열) 형태로 표현함으로써, 사용자가 입력한 질문(Query)에 대하여 DB에 저장한 문서 조각/단락(Chunk)을 검색하여 가져올 때 유사도 계산 시 활용될 수 있음.
	 예시:
		 - 1번 단락: [0.1, 0.5, 0.9, ... , 0.1, 0.2]
		 - 2번 단락: [0.7, 0.1, 0.3, ... , 0.5, 0.6]
		 - 3번 단락: [0.9, 0.4, 0.5, ... , 0.4, 0.3]
		prompt: ~예측한 시장의 연평균 성장률은 어떻게 되나요? - [0.1, 0.5, 0.9, ... , 0.2, 0.4]
		→ 유사도 계산
			- **1번: 80%** ⇒ 선택! 
			- 2번: 30%
			- 3번: 25%

##### 런타임(RunTime) 단계 (RAG 수행 단계) 
![[Pasted image 20240724124029.png]]
1. 검색기(Retriever): 쿼리(Query) 를 바탕으로 DB에서 검색하여 결과를 가져오기 위하여 리트리버를 정의합니다. 리트리버는 검색 알고리즘이며(Dense, Sparse) 리트리버로 나뉘게 됩니다. Dense: 유사도 기반 검색, Sparse: 키워드 기반 검색
2. 프롬프트: RAG 를 수행하기 위한 프롬프트를 생성합니다. 프롬프트의 context 에는 문서에서 검색된 내용이 입력됩니다. 프롬프트 엔지니어링을 통하여 답변의 형식을 지정할 수 있습니다.
3. LLM: 모델을 정의합니다.(GPT-3.5, GPT-4, Claude, etc..)
4. Chain: 프롬프트 - LLM - 출력 에 이르는 체인을 생성합니다.

- 파란색 박스 ⇒ 유사도 높은 값들


