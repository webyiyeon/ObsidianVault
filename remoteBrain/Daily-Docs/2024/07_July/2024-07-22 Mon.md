---
wakeup🌞: 2024-07-22T06:50:00
sleep🌜: 2024-07-22T00:30:00
mood: 😱
workout🏋️: 
workout-type: 
workout-routine: 
gratitude🙏: 
keyword🗝️: 
breakfast🍳: 
lunch🍚: 
dinner🥗: 
snack🍬: 
tags:
  - meal-log📝
  - study-log📓
  - workout-log💪
  - routine
---

🔺 [[remoteBrain/Daily-docs/2024/07_July/2024-07-21 Sun | 2024-07-21 Sun]]
🔻 [[remoteBrain/Daily-docs/2024/07_July/2024-07-23 Tue | 2024-07-23 Tue]]
___
<h1> <center>⏰TimeTable </center> </h1>

```gEvent
type: week
date: 2024-07-22 Mon
navigation: false
showAllDay: true
hourRange: [8, 24]
offset: -2
include: ["Default", "Todoist", "Korea", "Work", "Study", "WOOYEON"]
timespan: 7
```

--- 


# Routine 

- [ ] 영양제 챙겨 먹기 🔼 
- [ ] 운동하기 🔼
- [ ] 아침, 점심, 간식, 저녁 건강하게 4끼 챙겨먹기
- [ ] 무지출 챌린지 
- [ ] 집 정리·정돈하기 🔼
- [ ] 개발 공부하기
- [ ] 바깥 음식 안 먹기 
- [ ] 영어 공부하기 🔼 


# To-do List

- [x] 12:30 - 13:30 #work💼 엘라스틱서치 pull request ✅ 2024-07-22
- [ ] 14:00 - 18:00 #work💼 LLM 줌 강의
- [ ] 15:00 - 16:00 #work💼 개발 서버 반영 및 테스트 시나리오 요청

# Overdue List
```tasks
not done
(tags include #work💼) OR (tags include #chores🧺) OR (tags include #todo)
path does not include 2024-07-22 Mon
hide backlink
```

# Related Pages



# Thoughts & Inspirations

![[Pasted image 20240722121930.png]]
아아아아아악



### 토큰(Token)
토큰(Token)은 자연어 처리 (NLP)에서 텍스트를 작은 단위로 나누어 처리하기 위해 사용되는 기본 단위입니다. 단어, 부분 단어, 문자 등이 토큰이 될 수 있습니다.
	LLM(대형 언어 모델)에서 토큰은 텍스트 데이터를 모델이 이해하고 처리하기 위해 분할된 기본 단위입니다. 텍스트를 토큰으로 나누는 과정을 '토큰화'라고 합니다.
#### 토큰화의 방법
토큰화 방법에는 여러 가지가 있으며, 사용하는 방법에 따라 토큰의 정의가 달라질 수 있습니다.
- 문자 기반 토큰화: 텍스트를 문자 단위로 나누는 방법
	- 예시: "Hello" → ["H", "e", "l", "l", "o"]
- 단어 기반 토큰화: 텍스트를 단어 단위로 나누는 방법
	- 예시: "Hello, world" → ["Hello", ",", "wordl", "!"]
- **서브워드 기반 토큰화**: 단어를 더 작은 단위(subword)로 나누는 방법. **자주 사용되는 서브워드**를 기준으로 분할
	- 예시: "unhappiness" → ["un", "happiness"]
	- [참고] BPE(Byte Pair Encoding): 자주 등장하는 문자 쌍을 합쳐가며 서브워드를 생성하는 알고리즘 
	==GPT → 다음 Token 을 예측하는 것 → hallucination (거짓말, 헛소리)==

#### 토큰의 중요성
토큰은 모델이 텍스트를 이해하고 처리하는데 핵심적인 역할을 합니다. 토큰화의 결과에 따라 모델의 성능이 크게 영향을 받을 수 있습니다. 잘 정의된 토큰화 방법을 사용하면 모델이 텍스트의 의미를 더 정확하게 파악할 수 있습니다.
- **문맥 이해**: 모델이 문맥을 이해하고 적절하게 응답할 수 있게 도와준다.
- **효율성**: 적절한 크기의 토큰을 사용함으로써 연산 자원을 효율적으로 사용할 수 있다.

